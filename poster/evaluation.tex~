\section{Evaluation: CHANGE ME -- might be 'Case Studies' or 'User Studies' or
'Results' (if a controlled experiment) or just 'Evaluation'} % or "Research Plan"
\label{sec:vis}

Describe your evaluation and/or results here. 

\subsection{Usage Scenario}
\label{sec:usage}

This subsection is where we deviate from the format so the instructor has a
better idea of what you've done -- if designing a new visualization, describe
a usage scenario here. If your project is an experiment, an earlier section
should describe what you intend the users to do and/or what the users did in
your pilot study. You should still describe your ideal plan for evaluation
below. If you have case studies or user studies, put this subsection in the
visual design section instead. 

\subsection{Evaluation Plan}
\label{sec:plan}

As we stated in the original proposal, an intermediate evaluation test would be comparing documented dependency to the overview drawn by our tool. The overview should be a superset of documented dependency but not close to a universal set. We can worry less about false positives because the prototype doesn't have a high density of highlighted cells.

The above test can be automated relatively easily. We may not even need to get user involved. The ratio of identified documented dependency over all documented dependency should be the prime and only thing to look at in my opinion.

After that, an evaluation of false positives may be carried out. I see two possibilities: first is to contact the academical advisor from our dataset's university to confirm the possible dependency revealed by our tool; second is to find a local academical advisor who is willing to share her data and apply our tool to it. In fact, doing both would be more convincing.

Eventually, a user study should be done. We should have an academical advisor's feedback and improved the tool at this moment. We then ask the academical advisor to use this tool with students seeking advice, when they choose their courses. We need to be careful about the user groups here. Do we want to work with the same academical advisor all the time? Do we care about gender balance, student year(freshman, senior) balance?

I think we should share this tool with a handful of academical advisors, to avoid the tool being too personalized. When their feedback conflicts, we record their argument first and ask them if they are willing  to discuss further. We end up either solving the issue or having one implementation but understanding the alternatives. 

For a balanced student group, the question boils down to who's going to actually use this tool. If the tool is going to be used sololy with both student and academical advisor in presence, then we should accept whoever comes to the office and don't worry about balance at all. If the tool is going to be offered in the school system open to all students, we should invite students to evaluate it online in addition to academical office deployment. I'm more inclined to the first approach simply because it is easier to come true.
